[{"categories":["leetcode","go","c++"],"content":"求质数的模板","date":"2023-07-06","objectID":"/%E8%B4%A8%E6%95%B0/","tags":["刷题","leetcode"],"title":"质数","uri":"/%E8%B4%A8%E6%95%B0/"},{"categories":["leetcode","go","c++"],"content":" 质数又称素数。 一个大于1的自然数，除了1和它自身外，不能被其他自然数整除的数叫做质数； 本文介绍三种求质数的方法，第一种是基于常规求解的改进，第二种和第三种则是求质数的两种特殊方法。 ","date":"2023-07-06","objectID":"/%E8%B4%A8%E6%95%B0/:0:0","tags":["刷题","leetcode"],"title":"质数","uri":"/%E8%B4%A8%E6%95%B0/"},{"categories":["leetcode","go","c++"],"content":"第一种：简单遍历 遍历循环所有情况 ps : 因为因数是成对出现的，一个小于等于算数平方根，另外一个大于等于算数平方根。 代码如下： vector\u003cint\u003e prime(int n) { vector\u003cint\u003e res; for (int i = 0; i \u003c= n; i++) { int flag = 0; for (int j = 0; j \u003c= sqrt(i); j++) { if (i % j == 0) { flag = 1; break; } } if (flag == 0) { res.push_back(i); } } return res; } func prime(n int) (res []int) { for i := 2; i \u003c= n; i++ { flag := 0 for j := 2; j \u003c= int(math.Sqrt(float64(i))); j++ { if i%j == 0 { flag = 1 break } } if flag == 0 { res = append(res, i) } } return } ","date":"2023-07-06","objectID":"/%E8%B4%A8%E6%95%B0/:1:0","tags":["刷题","leetcode"],"title":"质数","uri":"/%E8%B4%A8%E6%95%B0/"},{"categories":["leetcode","go","c++"],"content":"第二种：埃氏筛 OI Wiki 考虑这样一件事情：对于任意一个大于$i$的正整数 ，那么它的$x$倍就是合数（$x\u003e1$)。利用这个结论，我们可以避免很多次不必要的检测。如果我们从小到大考虑每个数，然后同时把当前这个数的所有（比自己大的)倍数记为合数，那么运行结束的时候没有被标记的数就是素数了。 利用图来表示就是 从头遍历，将该数的倍数全部划去，例如2划去4,6,8,10,12，而到3时继续划去6,9,12，4由于被划去不进行遍历，5没被前面的数划掉，所以也是质数，以此类推，13以内的数可以得出2,3,5,7,11,13为质数。 代码如下： vector\u003cbool\u003e prime(int n){ vector\u003cbool\u003e isPrime(n + 1, true); isPrime[0] = isPrime[1] = false; for(int i = 2; i \u003c= n; i++){ if(isPrime[i]) { if((long long)i * i \u003c= n) for(int j = i * i; j \u003c= n; j += i) isPrime[j] = false; } } return isPrime; } func prime(n int) (res []bool) { res = make([]bool, n+1) for i := range res { res[i] = true } res[0], res[1] = false, false for i := 2; i \u003c= n; i++ { if res[i] { if i*i \u003c= n { for j := i * i; j \u003c= n; j += i { res[j] = false } } } } return 时间复杂度为$O(n\\log{\\log{n}})$ ","date":"2023-07-06","objectID":"/%E8%B4%A8%E6%95%B0/:2:0","tags":["刷题","leetcode"],"title":"质数","uri":"/%E8%B4%A8%E6%95%B0/"},{"categories":["leetcode","go","c++"],"content":"第三种：线性筛（欧拉筛） 每个回合只被划掉一次，每次只被最小的质因子划去 每个数从小到大乘上已经存在的质数，若可以被某个数整除就跳出循环，那么每个已经存在的质数都是x的最小质因子 vector\u003cint\u003e prime(int n){ vector\u003cint\u003e res; vector\u003cbool\u003e isPrime(n + 1, true); for(int i = 2; i \u003c= n; i++){ if(isPrime[i]){ res.push_back(i); } for(auto a : res){ if(a * i \u003e n) break; isPrime[a*i] = false; if(i % a == 0) break; } } return res; } func prime(n int) (res []int) { isPrime := make([]bool, n+1) for i := range isPrime { isPrime[i] = true } for i := 2; i \u003c= n; i++ { if isPrime[i] { res = append(res, i) } for _, x := range res { if x*i \u003e n { break } isPrime[x*i] = false if i%x == 0 { break } } } return 时间复杂度为$O(n)$ ","date":"2023-07-06","objectID":"/%E8%B4%A8%E6%95%B0/:3:0","tags":["刷题","leetcode"],"title":"质数","uri":"/%E8%B4%A8%E6%95%B0/"},{"categories":["documentation","JAVA"],"content":"注解与反射","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"注解与反射 这一章节对理解框架十分重要，包括接下来的Spring、Mybatis 文章参考以下内容，转载或引用请注明原作者和引用文章 https://www.bilibili.com/video/BV1p4411P7V3 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:0:0","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"注解（Annotation） ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:0","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"简介 注解是JDK 5.0开始引入技术（Annotation） 注解的作用 对程序作介绍（注释是对人来用的） 被其他程序使用 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:1","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"内置注解 @Override 在Java.lang.Override，声明重写超类后的第二个方法，作用范围：方法 @Deprecated 作用范围：方法，属性，类 表示不推荐使用此元素 @SuppressWarnings 用来抑制编译警告 与前俩个不同的是，此注解需要添加一个参数 @SuppressWarnings(“all”) @SuppressWarnings(“unchecked”) @SuppressWarnings(value ={“unchecked”,“deprecation”}) ….. ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:2","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"元注解 作用：负责注解其他注解，Java定义了四个标准类型 这些类型和它们所支持的类在Java.lang.Override包中可以找到 .( @Target , @Retention ,@Documented , @Inherited ) @Target：描述注解使用范围,所有范围如下 @Retention : 表示需要在什么级别保存该注释信息 , 用于描述注解的生命周期 (SOURCE \u003c CLASS \u003c RUNTIME) @Document：说明该注解将被包含在Javadoc中 @Inherited：说明子类可以继承父类中的该注解J ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:3","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"自定义注解 使用 @interface自定义注解时 , 自动继承了java.lang.annotation.Annotation接口 分析 @ interface用来声明一个注解 , 格式 : public @ interface 注解名 { 定义内容 } 若public不写默认为default 其中的每一个方法实际上是声明了一个配置参数. 方法的名称就是参数的名称. 返回值类型就是参数的类型 ( 返回值只能是基本类型,Class , String , enum ). 如果只有一个参数成员 , 一般参数名为value，通常这个参数名不需要在使用注解时表明 注解元素必须要有值 , 我们定义注解元素时 , 经常使用空字符串,0作为默认值 . 当默认值为-1时表示不存在 import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; public class Test3 { @MyAnnotation1(name = \"CodeForce\") public void test(){ } } @Target({ElementType.TYPE,ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @interface MyAnnotation1{ String name() default \"\"; int id() default -1 ;//-1代表不存在 等同于字符串的indexof()返回-1 String value() default \"\"; } ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:4","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射读取注解 利用反射读取注解信息三步骤： 定义注解 在类中使用注解 使用反射获取注解，一般此步骤都由框架集成，以下手动实现 import java.lang.annotation.*; import java.lang.reflect.Field; public class test5 { public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException { //通过反射获取类全部信息，所有东西 Class c1 = Class.forName(\"Person\"); //得到这个类的注解 Annotation[] annotations = c1.getAnnotations(); for (Annotation annotation : annotations) { System.out.println(annotation); } //获得类的注解value值 OnTable annotation = (OnTable) c1.getAnnotation(OnTable.class); System.out.println(annotation.value()); //获得类指定注解的值 Field name = c1.getDeclaredField(\"name\"); OnFiled onFiled = name.getAnnotation(OnFiled.class); System.out.println(onFiled.columnName()+\"---\"+onFiled.type()+\"---\"+onFiled.length()); //此方法就是通过得到信息,在JDBC生成相关SQL语句 } } @OnTable(\"db_person\") class Person{ @OnFiled(columnName = \"db_name\",type = \"varchar\",length = 3) private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return \"Person{\" + \"name='\" + name + '\\'' + '}'; } } @Target(value = {ElementType.FIELD}) @Retention(value = RetentionPolicy.RUNTIME) @interface OnFiled{ String columnName(); //列名 String type(); //类型 int length();//长度 } @Target(value = {ElementType.TYPE}) @Retention(value = RetentionPolicy.RUNTIME) @interface OnTable{ String value();//只有一个推荐使用value } ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:1:5","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射（Reflection） ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:0","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"动态语言与静态语言： Java虽然作为静态语言，但是利用反射机制使得Java拥有部分动态语言的特性 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:1","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射 Reflection（反射）是Java被视为动态语言的关键，反射机制允许程序在执行期借助于Reflection API 取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。 Class c = Class.forName(\"java.util.List\"); 加载完类之后，在堆内存的方法区中就产生了一个Class类型的对象（一个类只有一个Class对象），这个对象就包含了完整的类的结构信息。我们可以通过这个对象看到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，我们形象的称之为：反射 Java反射提供的功能 在运行时判断任意一个对象所属的类 在运行时构造任意一个类的对象 在运行时判断任意一个类所具有的成员变量和方法 在运行时获取泛型信息 在运行时调用任意一个对象的成员变量和方法 在运行时处理注解(框架注解开发的原理) 生成动态代理 ……. Java反射优点和缺点 优点：可以实现动态创建对象和编译，体现出很大的灵活性 ! 缺点：对性能有影响。使用反射基本上是一种解释操作，我们可以告诉JVM，我们希望做什么并且它满足我们的要求。这类操作总是慢于直接执行相同的操作。 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:2","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射相关的主要API java.lang.Class : 代表一个类 java.lang.reflect.Method : 代表类的方法 java.lang.reflect.Field : 代表类的成员变量 java.lang.reflect.Constructor : 代表类的构造器 …… ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:3","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"Class类 如果还得的话，Object作为所有类的基类有个getClass()方法 public final native Class\u003c?\u003e getClass(); 以上的方法返回值的类型是一个Class类，此类是Java反射的源头，实际上所谓反射从程序的运行结果来看也很好理解，即：可以通过对象反射求出类的名称。 fds\r对象照镜子后可以得到的信息：某个类的属性、方法和构造器、某个类到底实现了哪些接口。对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个结构(class/interface/enum/annotation/primitive type/void/[])的有关信息。 Class 本身也是一个类 Class 对象只能由系统建立对象 一个加载的类在 JVM 中只会有一个Class实例 一个Class对象对应的是一个加载到JVM中的一个.class文件 每个类的实例都会记得自己是由哪个 Class 实例所生成 通过Class可以完整地得到一个类中的所有被加载的结构 Class类是Reflection的根源，针对任何你想动态加载、运行的类，唯有先获得相应的Class对象 获取class几种方式 还有一种利用类加载器在下文中会提到 方法名 功能说明 static ClassforName(String name) 返回指定类名name的Class对象 Object newInstance() 调用缺省构造函数，返回Class对象的一个实例 getName() 返回此Class对象所表示的实体（类，接口，数组类或void）的名称。 Class getSuperClass() 返回当前Class对象的父类的Class对象 Class[] getinterfaces() 获取当前Class对象的接口 ClassLoader getClassLoader() 返回该类的类加载器 Constructor[] getConstructors() 返回一个包含某些Constructor对象的数组Method getMothed( Method getMothed(Stringname,Class.. T) 返回一个Method对象，此对象的形参类型为paramType Field[] getDeclaredFields() 返回Field对象的一个数组 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:4","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"哪些类型可以有class对象 class：外部类，成员(成员内部类，静态内部类)，局部内部类，匿名内部类。 interface：接口 []：数组 enum：枚举 annotation：注解@interface primitive type：基本数据类型 void 这里要注意一点的是数组只要元素类型与维度一样，就是同一个class ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:5","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"Java内存分析 类的加载过程 类的加载与ClassLoader的理解 加载 将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的java.lang.Class对象. 链接：将Java类的二进制代码合并到JVM的运行状态之中的过程。 验证：确保加载的类信息符合JVM规范，没有安全方面的问题 准备：正式为类变量（static）分配内存并设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配。 解析：虚拟机常量池内的符号引用（常量名）替换为直接引用（地址）的过程。 初始化： 执行类构造器()方法的过程。类构造器()方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的。（类构造器是构造类信息的，不是构造该类对象的构造器）。 当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。 虚拟机会保证一个类的()方法在多线程环境中被正确加锁和同步。 什么时候会发生类初始化？ 类的主动引用（一定会发生类的初始化） main在虚拟机启动时的初始化 new 一个类的对象 调用类的静态成员（除了final常量）和静态方法 对类进行反射调用 当初始化一个类，如果父类没有被初始化，则会先初始化它的父类 类的被动引用（不会发生类的初始化） 当访问一个静态域时，只有真正声明这个域的类才会被初始化。如：当通过子类引用父类的静态变量，不会导致子类初始化 通过数组定义类引用，不会触发此类的初始化 类名[] 名称 = new 类名[数量];//不会初始化此类 引用常量不会触发此类的初始化（常量在链接阶段就存入调用类的常量池中了） 类加载器的作用 类加载的作用：将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后在堆中生成一个代表这个类的java.lang.Class对象，作为方法区中类数据的访问入口。 类缓存：标准的JavaSE类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载（缓存）一段时间。不过JVM垃圾回收机制可以回收这些Class对象 类加载器作用是用来把类(class)装载进内存的。JVM 规范定义了如下类型的类的加载器 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:6","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"创建运行时类的对象 通过反射获取运行时类的完整结构 Field、Method、Constructor、Superclass、Interface、Annotation 实现的全部接口 所继承的父类 全部的构造器 全部的方法 全部的Field 注解 …… ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:7","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"有了Class对象，能做什么事情 创建类的对象：调用Class对象的newInstance()方法 类必须有一个无参数的构造器。 类的构造器的访问权限需要足够 若没有无参构造器，则按照以下方式创建 通过Class类的getDeclaredConstructor(Class … parameterTypes)取得本类的指定形参类型的构造器 向构造器的形参中传递一个对象数组进去，里面包含了构造器中所需的各个参数。 通过Constructor实例化对象 //c1是类getclass得来的 //无参构造 Person person = (Person)c1.newInstance(); //有参构造 Constructor constructor = c1.getDeclaredConstructor(String.class); Person person1 = (Person) constructor.newInstance(\"CodeForce\"); System.out.println(person1); 调用指定方法 通过反射，调用类中的方法，通过Method类完成。 通过Class类的getMethod(String name,Class…parameterTypes)方法取得一个Method对象，并设置此方法操作时所需要的参数类型。 之后使用Object invoke(Object obj, Object[] args)进行调用，并向方法中传递要设置的obj对象的参数信息。 //通过反射获取一个方法(反射操作方法) Method setName = c1.getDeclaredMethod(\"setName\", String.class); setName.invoke(person,\"Code\"); System.out.println(person.getName()); Object 对应原方法的返回值，若原方法无返回值，此时返回null 若原方法若为静态方法，此时形参Object obj可为null 若原方法形参列表为空，则Object[] args为null 若原方法声明为private,则需要在调用此invoke()方法前，显式调用方法对象的 setAccessible(true)方法，将可访问private的方法。 Field name = c1.getDeclaredField(\"name\"); //name作为private属性，正常不能直接访问，可通过setAccessible关闭检测 name.setAccessible(true); name.set(person,\"code1\"); System.out.println(person.getName()); ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:8","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"setAccessible Method和Field、Constructor对象都有setAccessible()方法。 setAccessible作用是启动和禁用访问安全检查的开关。 参数值为true则指示反射的对象在使用时应该取消Java语言访问检查。 提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为true。 使得原本无法访问的私有成员也可以访问 参数值为false则指示反射的对象应该实施Java语言访问检查 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:9","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射操作泛型 Java采用泛型擦除的机制来引入泛型 , Java中的泛型仅仅是给编译器javac使用的,确保数据的安全性和免去强制类型转换问题 , 但是 , 一旦编译完成 , 所有和泛型有关的类型全部擦除 为了通过反射操作这些类型 , Java新增了 ParameterizedType , GenericArrayType , TypeVariable和 WildcardType 几种类型来代表不能被归一到Class类中的类型但是又和原始类型齐名的类型. ParameterizedType : 表示一种参数化类型,比如Collection GenericArrayType : 表示一种元素类型是参数化类型或者类型变量的数组类型 TypeVariable : 是各种类型变量的公共父接口 WildcardType : 代表一种通配符类型表达式 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:10","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["documentation","JAVA"],"content":"反射操作注解 详情请看反射读取注解那段 ","date":"2020-03-06","objectID":"/documentation-reflect-and-annotation/:2:11","tags":["JAVA"],"title":"JAVA 注解与反射","uri":"/documentation-reflect-and-annotation/"},{"categories":["template","blog"],"content":"新建博客模板","date":"2020-03-06","objectID":"/template/","tags":["template"],"title":"博客模板","uri":"/template/"},{"categories":["template","blog"],"content":"本文仅仅只是用作个人的博客的新建文章 hugo new posts/index.zh-cn.md ","date":"2020-03-06","objectID":"/template/:0:0","tags":["template"],"title":"博客模板","uri":"/template/"},{"categories":["AI","documentation"],"content":"Improving Language Understanding by Generative Pre-Training 论文阅读","date":"2019-10-01","objectID":"/improving-language-understanding-by-generative-pre-training/","tags":["AI","GPT"],"title":"  GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读","uri":"/improving-language-understanding-by-generative-pre-training/"},{"categories":["AI","documentation"],"content":"GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读 Improving Language Understanding by Generative Pre-Training ","date":"2019-10-01","objectID":"/improving-language-understanding-by-generative-pre-training/:0:0","tags":["AI","GPT"],"title":"  GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读","uri":"/improving-language-understanding-by-generative-pre-training/"},{"categories":["AI","documentation"],"content":"文献简介： ​ 《Improving Language Understanding by Generative Pre-Training》这篇文献是由OpenAI的研究团队于2018年6月11日上发表的，截至2023年4月,Google Scholar显示该论文的学术引用量达到5254次，这篇文献是一篇技术报告，没有发表在期刊或会议上，因此没有期（卷）或分区评价。 ​ 这篇文献的主要贡献是提出了一种基于生成式预训练的语言理解模型，该模型可以在各种自然语言处理任务上取得显著的性能提升。 ​ 这篇文献的核心思想是先在大规模的无标注文本上预训练一个语言模型，然后在每个具体任务上进行判别式微调，同时利用任务相关的输入变换来实现有效的迁移学习。 ","date":"2019-10-01","objectID":"/improving-language-understanding-by-generative-pre-training/:1:0","tags":["AI","GPT"],"title":"  GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读","uri":"/improving-language-understanding-by-generative-pre-training/"},{"categories":["AI","documentation"],"content":"文献内容总结： ​ 该文献指出，在当时的NLP领域，与计算机视觉相比，缺乏大量的标注数据集。然而，无标注的文本语料库却非常丰富。因此，作者首先在大量的非标注语料库中进行生成式预训练（Generative Pre-Training），然后针对每个特定的任务进行区分性微调（Discriminative Fine-Tuning）。这种方法使得模型能够在不同的NLP任务上取得很好的表现。 ​ 该论文采用了半监督（Semi-Supervised）的训练方式，即通过无监督学习进行预训练，再通过监督学习进行微调。在评估了RNN网络和transformer网络之后，作者发现后者可以很好地捕捉较长的语言结构，从而使得模型在处理子任务时具有更好的泛化性。这种方法为模型的训练带来了很好的效果。 无监督学习下的预训练（Pre-Training） ​ 通过输入文本片段 $\\mathcal{U}={u_1,\\ldots,u_n}$，其中$u_1$可以表示为单个字符，$U$可以表示为一个包含许多字符的字符串，根据前$k$个词来预测下一个词$u_i$的概率$P\\left(u_i\\middle| u_{i-k},\\ldots,u_{i-1}\\right)$，然后最大化似然函数来进行训练。目标函数如下所示： $$ L_1\\left(\\mathcal{U}\\right)=\\sum_{i=k+1}^{n}{\\log{P}\\left(u_i\\middle| u_{i-k},\\ldots,u_{i-1};\\Theta\\right)} $$ ​ 其中 $k$ 表示给定$k$个词的情况下，通过模型 $\\Theta $预测出$u_i $的概率，并将所有出现的词的概率取对数相加作为文本出现的联合概率。预训练的目标即为优化该$L_1$目标函数。此外，模型仅使用了Transformer[2]的解码器（Decoder）进行预训练。预测过程涉及将n个词进行词嵌入（Word Embedding），然后加上位置嵌入（Position Embedding）。接下来，文本序列通过多层 Transformer 块进行处理，并在最后一层 Transformer 块后进行最后一次投影。最后，经由 Softmax 操作，输出文本中每个词的概率分布。 监督学习下的微调(fine-tuning) ​ 在进行大量无标注的文本语料库训练后，模型还需要在某些特定的目标任务下进行微调。给定文本$ x^1,\\ldots,x^m$和相应的标注信息$y$，将它们输入到预训练模型中进行微调。在微调过程中，$h_l^m $表示最后一层 Transformer 块的输出，$W_y$表示最后一层输出层的参数。 $$ P\\left(y\\mid x^1,\\ldots,x^m\\right)=softmax\\left(h_l^mW_y\\right) \\ L_2\\left(\\mathcal{C}\\right)=\\sum_{\\left(x,y\\right)}{\\log{P}\\left(y\\mid x^1,\\ldots,x^m\\right)} $$ ​ 但是作者并不仅仅只将$L_2$作为微调阶段的目标函数，而是采用$L_3\\left(\\mathcal{C}\\right)=L_2\\left(\\mathcal{C}\\right)+\\lambda\\ast L_1\\left(\\mathcal{C}\\right)$方式，将预训练模型的目标函数加权求和。 特定任务下的微调： ​ 图1展示了文中说明的四种特定任务，通过添加的线性层(Linear)，也是上文说的W_y参数，并且不修改transformer的结构来进行微调。 分类 (Classification)：输入开始符(Start)，文本(Text)，抽取符(Extract)，线性层即输出分类数。 包含(Entailment)：输入开始符(Start)，文本(Text)，分隔符(Delim)，假设(Hypothesis)，抽取符(Extract)，线性层输出类似包含，不包含，无关三分类。 相似(Similarity)：输入开始符(Start)，文本(Text 1)，分隔符(Delim)，文本(Text 2)，抽取符(Extract)；输入开始符(Start)，文本(Text2)，分隔符(Delim)，文本(Text1)，抽取符(Extract)。经过相加后进入线性层，输出两端文本在不同语境下是否相似的。 多选(Multiple Choice)：输入开始符(Start)，文本(Context)，分隔符(Delim)，答案(Answer 1)，抽取符(Extract)；输入开始符(Start)，文本(Context)，分隔符(Delim)，答案(Answer N)，抽取符(Extract)；每个输入都经过一个线性层而后通过softmax操作求出每个答案的置信度。 实验 ​ 最后实验是在包含7000篇没有发表的书籍上进行训练，使用12层的transformer解码器，每一层包含768维的网络进行训练，使用了3072维数的FFN(Position-wise Feed-Forward Network)层，使用了Adam优化器和GELU激活函数，其中使用了4000个合并的[3]字节对编码(BPE)，并且在位置嵌入使用了自学习的位置嵌入(learned position embedding)。 ","date":"2019-10-01","objectID":"/improving-language-understanding-by-generative-pre-training/:2:0","tags":["AI","GPT"],"title":"  GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读","uri":"/improving-language-understanding-by-generative-pre-training/"},{"categories":["AI","documentation"],"content":"文献研究内容思考： 为什么GPT模型的网络结果必须采用解码器 GPT预训练模型仅仅只采用transformer的解码器(decoder)，是源于解码器中的第一层多头注意力(Multi-Head Attention)采用了掩码(Masked)的操作，具体操作类似于输入图像只裁剪中间信息，将部分像素通过与掩码矩阵相乘进行消除，使得输出时只能获取未被掩码掉的部分，而论文中采用该方法，将k作为上下文窗口，涂抹掉信息来预测下一个词。这种掩码操作被称为自回归性质，它可以确保模型在生成序列时遵循语言的线性顺序，而不会出现随意跳跃或重复生成的情况。这样，通过大量的文本数据来预训练该模型，可以使其学习到自然语言中的语法结构、词汇等知识。 为什么GPT的位置嵌入没有使用transformer的位置编码 GPT的位置嵌入并没有使用原transformer的正弦函数(sinusoidal)的方式求得。其中transformer提出的方式见如下公式。经过查阅，发现Bert[4]也不是采用transformer的正弦函数，可能自学习的位置嵌入更加贴合数据集，并且目前的单一输入文本长度也不会过长，但在将来越来越大的数据集下，这种可以扩展到无限长度的方式本身具有的泛化性可能会更加有优势。 $$ PE_{\\left(pos,2i\\right)}=\\sin{\\left(pos/{10000}^{2i/d}\\right)}\\ PE_{\\left(pos,2i+1\\right)}=\\cos{\\left(pos/{10000}^{2i/d}\\right)} $$ 结论 ​ GPT这篇论文奠定了ChatGPT的基础，GPT2论文和GPT3论文主是对预训练方式的改进和不断地增大模型的参数，Instruct GPT[5]提出的RLHF, 就是基于人类反馈（Human Feedback）对语言模型进行强化学习（Reinforcement Learning），人工训练出一个符合人类行为或反馈的奖励模型，然后利用PPO算法进行语言模型的微调，即以强化学习方式依据人类反馈优化语言模型，自此ChatGPT才得以从量的积累到质的改变，并且我猜测这也是ChatGPT能很好避开违法犯罪、种族歧视等话题的一种最为有效的手段。 参考文献： Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30. Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.“arXiv preprint arXiv:1508.07909(2015). Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018. Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback, 2022[J]. URL https://arxiv.org/abs/2203.02155. Lambert, et al., “Illustrating Reinforcement Learning from Human Feedback (RLHF)”, Hugging Face Blog, 2022. URL https://huggingface.co/blog/rlhf 参考资料： GPT，GPT-2，GPT-3 论文精读【论文精读】 ","date":"2019-10-01","objectID":"/improving-language-understanding-by-generative-pre-training/:3:0","tags":["AI","GPT"],"title":"  GPT的前身：Improving Language Understanding by Generative Pre-Training 论文阅读","uri":"/improving-language-understanding-by-generative-pre-training/"},{"categories":["c++","documentation"],"content":"c++引用","date":"2019-10-01","objectID":"/c-/","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"C++ 之引用（reference） c++提供很多内存模型，甚至是很多地方可以存放对象(堆，栈，全局数据区)，以及很多访问对象的方式，下文就介绍其中一种方式：引用。 本文只是记录了引用学习过程，参考资料：浙江大学授课 ","date":"2019-10-01","objectID":"/c-/:0:0","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"引用基本格式 引用的基本格式为 type \u0026 refname = name;,其中name必须是个实际的名字，且==必须经过初始化==，以下是三种访问数值的方式： char c;//一个普通字符 char *p = \u0026c;//利用指针访问字符； char \u0026r = c;//利用引用来访问字符 通过声明一个新的名字给存在的对象,并且通过引用的方式修改值 int x = 1; int \u0026y = x; cout \u003c\u003c \"y = \" \u003c\u003c y; //输出y = 1； y = 2; cout \u003c\u003c \"x = \"\u003c\u003c x;//输出x = 18; ","date":"2019-10-01","objectID":"/c-/:1:0","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"引用规则 使用引用的规则，但定义引用类型时==必须被初始化==，初始化会建立一种绑定的关系 //声明 int x = 3; int \u0026y = x; const int \u0026z =x; //作为函数参数 void f(int \u0026x); f(y);//当函数调用时初始化 const表示通过z不得修改x，x可以做任意改变，其中绑定关系即使不存在const也是默认绑定(同const int *p = x有着异曲同工的作用，即表示x无法通过p这个指针被修改,) 关于void f(int \u0026x);这里传入的是 x 的引用（说明此处传入就是 f(y) 里的 y ），也就是说如果调用f的函数，那么在函数内部可以直接更改y的值，而调用者无法通过调用方式来察觉值的变化 引用的绑定是无法在运行时被改变，这也就是和指针区别之一 引用的目标必须有一个临时存放的位置，若传入参数没有变量名，则会出现下述代码编译情况 void func(int \u0026); func(i*3);//警告或者报错 ","date":"2019-10-01","objectID":"/c-/:2:0","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"引用在函数中作用 int* f(int* x) { (*x)++; return x; } int\u0026 g(int\u0026 x) { x++; return x; } int x; int\u0026 h() { int q; //return q; //错误，无法返回一个地址值 return x;//正确，x作为全局变量始终存在 } int main() { int a = 0; f(\u0026a);//a=1 g(a);//a=2 h() = 16;// == 16赋值给x的引用，所以x=16 } ","date":"2019-10-01","objectID":"/c-/:2:1","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"指针对比引用 引用 指针 不能为空 可以设置为空 绑定导致无法指向新的地址 可以指向新的地址 需要依赖一个存在的值，只是一个变量的别名 可以独立于存在的对象 无法使引用出引用 int a; int\u0026 b=a; int\u0026 c=b;//错误，不存在引用的引用 int\u0026 c=a//正确，此时b,c皆为a的引用 没有指针可以被引用 int\u0026* p是非法代码 指针可以去引用 void f(int*\u0026 p)是合法代码 没有数组的引用 ","date":"2019-10-01","objectID":"/c-/:3:0","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["c++","documentation"],"content":"引用的好处 其实相对于C++来说，引用保留了指针的好处，又减少了部分指针带来的安全隐患，更需要注意的时引用对于读取数据来说速度提升巨大，如果涉及到从高维数组读取大量数据时，在auto 后加入\u0026来引用数据进行操作对于AC的时间来说有十分大的帮助。 ","date":"2019-10-01","objectID":"/c-/:4:0","tags":["c++"],"title":" C++ 之引用（reference)","uri":"/c-/"},{"categories":["AI","documentation"],"content":"Deep Residual Learning for Image Recognition论文阅读","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"残差网络：Deep Residual Learning for Image Recognition论文阅读 Deep Residual Learning for Image Recognition ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:0:0","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"文献简介： 这篇论文就是resnet网络的开篇之作，为了解决更深的神经网络更难训练，作者提出了的残差结构，赢得了ILSVRC 2015分类任务的第一名，并且该网络比VGG网络深了八倍，不仅如此，还在物体监测，图像分割上取得了很好的结果。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:1:0","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"Deep Residual Learning for Image Recognition 文献内容总结： ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:0","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"介绍 ​ 深度卷积网络给图像分类任务带来了一系列的突破，网络越深，往往卷积神经网络（CNN）能够提取更多的低、中、高层的特征，而端到端的多层分类器更是可以通过堆叠深度来增加特征的“层次”。那么训练一个好的模型是否和我们假设的一样，堆很多的层数就够了？答案当然没有那么简单，一味地增加网络深度，会使得梯度消失或爆炸。但是解决这个问题目前已经存在一种较好的方法解决，那就是在网络权重初始化，选取合适的初始值，并且在网络的中间层加入一些正则化措施 ​ 即使解决了梯度爆炸和梯度消失的问题，网络退化问题又随之出现。总所周知，CNN层数越多，越容易出现过拟合，但是作者发现过拟合的并不是导致网络退化的主要原因，因为训练误差无法继续下降了（过拟合现象通常是指训练误差和测试误差相差过大，训练误差表现的更好）。 ​ 作者先尝试了在浅层网络后加入一个与之对应的恒等映射（identity mapping）层来实现深层的网络，即直接复制前一层的参数，来将网络加深，也就是输入输出是相等的，将权重分成多个部分。理论来说这样只是在浅层网络上扩展为深层，结果应该好于或等于浅层网络，但是实验发现这样这个想法并不可行。原因也很简单，梯度下降算法SGD（文中使用的算法）在多层网络下找不到全局最优解了，越来越深的网络必然使得全局最优解越来越难找到。（SGD的精髓就是能够一直能跑的动，如果哪一天跑不动了，梯度没了就完了，就会卡在一个地方出不去了，所以它的精髓就在于需要梯度够大，要一直能够跑，因为有噪音的存在，所以慢慢的他总是会收敛的，所以只要保证梯度一直够大，其实到最后的结果就会比较好）。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:1","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"残差网络 ​ 作者就此提出了深度残差学习框架，如下图所示，假设$x$ 是来自浅层网络，在原有的结构中加入了右边的一条链接，实际上这被称为快捷链接（shortcut connection），直接跳过包含权重的两层网络，将$x$输出到下一次激活函数内。 ​ 在普通的卷积神经网络（CNN）中，我们要输入$x$，然后输出$h(x)$，即$x -\u003e f(x) -\u003e h(x) $，$f(x)$的参数就是我们需要拟合的对象（也就是神经网络拟合的某一层的权重）。但是残差网络中，变成了$x - \u003e f(x) + x -\u003e h(x)$，那么网络拟合的函数不再是CNN中的$h(x)$而是$h(x) -x$ ，因为$f(x) = h(x) -x $，这也就是为什么被称为残差块（residual block）。 ​ 继续通过简单的证明来说明为什么这样可以消除网络的退化，如下推导过程所示，其中$A^l$是经过残差得来的上一层结果，$A^{l+2}$是输出结果，我们可以看到即使$f(x)$趋近于0，传入下一层的参数由于恒等映射的存在，依然可以保留非线性层的权重。 $$ A^{l+2} = relu(f(z^{l+2} ) + A^{l})\\\\ st: z^{l+2} = w^{l+2} * A^{l+2} + b^{l+2}\\\\ =\u003e A^{l+2} = relu(f(w^{l+2} * A^{l+2} + b^{l+2})+ A^{l})\\\\ 如果 f(z^{l+2} ) -\u003e 0, A^{l+2} -\u003e relu(A^{l})\\\\ 那么经过relu层，只要A^{l} \u003e 0 ，那么就有A^{l+2} -\u003e A^{l} $$ ​ 所以就算是在极端情况下，越来越深的网络可能存在一些权重近似为0的堆叠层，由于残差块的作用，我们依然可以保留权重继续优化我们的模型。 ​ 我们还需要考虑一个问题，那就是捷径链接和输出的维度不同该怎么办，那么我们就需要通过添加额外的$W_s$投影矩阵使得式子变成$y = F(x, {Wi}) + W_s\\cdot x $，令$F(x, {Wi})$维度和$ W_s\\cdot x $相同，其中$F(x, {Wi})$也代表了可以在跨越多个层进行链接，如图所示，当中间的层数为一时，就变成上文提出的式子。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:2","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"Bottleneck layer ​ 作者在实验中还设计了一个瓶颈层，包含了三层，分别是 1×1、3×3 和 1×1 卷积，其中 1×1 层负责减少然后增加（恢复）维度，使 3x3 层成为输入/输出维度较小的瓶颈，并且证明这两种设计具有相似的时间复杂度。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:3","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"开放性的问题 ​ 作者认为，深度模型的优化不太可能是由梯度消失引起的。他们训练了没有残差结构的plain网络，使用BN来保证了前向传播的信号具有非零的方差，并且验证了返现传播的梯度在BN中也起了作用，所以前向和反向传播都没有消失，实验结果表明34层的网络依然能取得不错的成绩，说明网络在一定程度上是正常运行的，所以作者猜测难以继续优化的原因是因为加深的网络使得减少训练误差的难度呈现指数级的上升。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:4","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"pytoch源码辅助理解 class BasicBlock(nn.Module): expansion: int = 1 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None, ) -\u003e None: super().__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\") if dilation \u003e 1: raise NotImplementedError(\"Dilation \u003e 1 not supported in BasicBlock\") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample # 下采样层 self.stride = stride def forward(self, x: Tensor) -\u003e Tensor: identity = x out = self.conv1(x) # 卷积层 out = self.bn1(out) # BN层 out = self.relu(out) # relu函数 out = self.conv2(out) # 卷积层 out = self.bn2(out) # BN层 if self.downsample is not None: identity = self.downsample(x) # 下采样，也就是Identity Mapping out += identity out = self.relu(out) # relu函数 return out class Bottleneck(nn.Module): expansion: int = 4 def __init__( self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., nn.Module]] = None, ) -\u003e None: super().__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.0)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x: Tensor) -\u003e Tensor: identity = x out = self.conv1(x) # 1x1 卷积层 out = self.bn1(out) # BN层 out = self.relu(out) # relu函数 out = self.conv2(out) # 3x3 卷积层 out = self.bn2(out) # BN层 out = self.relu(out) # relu函数 out = self.conv3(out) # 1x1 卷积层 out = self.bn3(out) # BN层 if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) # relu函数 return out ​ 网络的构造大家可以自行去查看源码，这里只给出残差单元的basic和Bottleneck的框架，pytorch源码阅读应该是相对较为简单，里面也提供了许多版本的resnet。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:2:5","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"文献研究内容思考： ​ 后续这篇论文作者通过分析残差块(residual block)背后的传播公式，通过一系列的消融实验(ablation experiments)来确定了恒等映射的重要性，并提出来新的残差单元(residual unit)，使得训练更加简单，并且提高模型的泛化能力。代码位于：https://github.com/KaimingHe/resnet-1k-layers。并且两篇论文均是来自微软研究院，都出自Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun。作者也设计了多种消融实验，并分析其结果，如下图所示，其中也通过推导来证明为什么一定要满足恒等映射。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:3:0","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"为什么模型不会出现过拟合？ ​ CIFAR数据集比较小，但是在1000层的网络架构下仍然没有过拟合。 ​ 尽管深度模型具有许多参数和复杂的结构，但由于其内在设计的缘故，模型的复杂度实际上并不高。事实上，引入残差连接可以降低模型的复杂度，从而减轻过拟合的问题。降低模型复杂度并不意味着无法表示其他信息，而是可以找到一个更简单的模型来拟合数据。例如，即使没有残差连接，理论上也可能学习到某种恒等变换，但实际上这是不可行的，因为没有指导整个网络的信息流向，从而导致难以实现。因此，手动将这种变换引入模型中，使其更易于训练，相当于降低了模型的复杂度。 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:3:1","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"正向与反向传播 图中$x,y,z,w$表示初始值，每一条线的绿色数字代表着正向传播的参数，红色代表的是反向传播的参数，每一个圆圈代表一次运算，其中的符号或数字代表着计算方式。 我再给出添加了残差块的例子，希望这个图可以帮助理解残差网络的反向传播 ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:3:2","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["AI","documentation"],"content":"总结 ​ 残差网络的提出到目前为止，已经有超过90%的网络在使用基于跳跃连接的网络，根据该结构的灵感，提出了各式各样的网络结果，例如 LSTNet中上，类似的设计了循环跳跃层来跳过部分信息流，在RNN处理时序相关的问题时，也提出了相似的结构来跳过部分时间帧。 参考文献： Identity Mappings in Deep Residual Networks 参考资料： ResNet论文逐段精读【论文精读】 Residual Blocks in Deep Learning pytorch的resnet源代码 Gradient backpropagation through ResNet skip connections https://cs231n.github.io/optimization-2/ ","date":"2019-10-01","objectID":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/:3:3","tags":["AI"],"title":" 残差网络：Deep Residual Learning for Image Recognition论文阅读","uri":"/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C_deep-residual-learning-for-image-recognition%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"categories":["reading","write"],"content":"《一句话顶一万句话》全书评 ","date":"2023-06-21","objectID":"/yi-wang-ju-ding-yi-ju/:0:0","tags":["刘震云"],"title":"《一句话顶一万句话》全书评","uri":"/yi-wang-ju-ding-yi-ju/"},{"categories":["reading","write"],"content":"内容简介 《一句顶一万句》的故事很简单，小说的前半部写的是过去：孤独无助的吴摩西失去唯一能够“说得上话”的养女，为了寻找，走出延津；小说的后半部写的是现在：吴摩西养女的儿子牛爱国，同样为了摆脱孤独寻找“说得上话”的朋友，走向延津。一走一来，延宕百年。书中的人物大部分是中国最底层的老百姓，偏偏安排了一个意大利牧师老詹。—–来源豆瓣 ","date":"2023-06-21","objectID":"/yi-wang-ju-ding-yi-ju/:1:0","tags":["刘震云"],"title":"《一句话顶一万句话》全书评","uri":"/yi-wang-ju-ding-yi-ju/"},{"categories":["reading","write"],"content":"作者 刘震云，男，1958年5月出生，河南新乡延津人。1973年参加中国人民解放军。1978年复员，在家乡当中学教师，同年考入北京大学中文系。1982年毕业到《农民日报》工作。1988年至1991年曾到北京师范大学，鲁迅文学院读研究生。1982年开始创作，1987年后连续发表在《人民文学》《塔铺》、《新兵连》、《头人》、《单位》、《官场》、《一地鸡毛》、《官人》、《温故一九四二》等描写城市社会的“单位系列”和干部生活的“官场系列”，引起强烈反响在这些作品中，他迅速表现出成为大作家的潜在能力，确立了创作中的平民立场，将目光集中于历史、权力和民生问题，但又不失于简洁直接的白描手法，也因此被称为“新写实主义”作家。其中《塔铺》获1987-1988全国优秀短篇小说奖。—–来源豆瓣 ","date":"2023-06-21","objectID":"/yi-wang-ju-ding-yi-ju/:2:0","tags":["刘震云"],"title":"《一句话顶一万句话》全书评","uri":"/yi-wang-ju-ding-yi-ju/"},{"categories":["reading","write"],"content":"精彩语录 世上的事情，原来件件藏着委屈！话，一旦成了人与人唯一沟通的东西，寻找和孤独便伴随一生。心灵的疲惫和生命的颓废，以及无边无际的茫然和累，便如影随形地产生了。 “日子是过以后，不是过从前。我要想不清楚这一点，也活不到今天。” 人是掰扯不得的，掰扯了别人，就是掰扯了自己。 “街上的事，一件事就是一件事；家里的事，一件事扯着八件事。你只给我说了一件事，我如何去断八件事呢？” “好把的是病，猜不透的是人心。” 事情想不明白，人的忧愁还少些；事情想明白了，反倒更加忧愁了。 不求人办事是熟人，一求人办事人就生了。 “一个人说正经话，说得不对可以劝他；一个人在胡言乱语，何劝之有？” 也许他肚子里有东西，但像茶壶里煮饺子一样，倒不出来。 躁人之辞多，吉人之辞寡 不患人之不己知，患不知人也 ","date":"2023-06-21","objectID":"/yi-wang-ju-ding-yi-ju/:3:0","tags":["刘震云"],"title":"《一句话顶一万句话》全书评","uri":"/yi-wang-ju-ding-yi-ju/"},{"categories":["reading","write"],"content":"书评 简述 全书分为上下两部，也包含了老杨家四代人的故事，爹老杨，二儿子杨百顺，孙女巧玲（曹青娥），曾孙牛爱国。 儿子杨百顺不是长子，家里排行老二，大小和老杨就不亲，老杨始终希望自己的豆腐铺能越开越好，所以自然和二儿子说不上话。杨百顺打小就崇拜喊丧的罗长礼，罗长礼是做醋的，但醋做的不好，喊丧却在县里有名。后面又遇到剃头的老裴，杀猪的老曾，传教的老詹。县里学堂免费招生，老杨听从他人言语，利用小把戏把杨百顺老老实实的骗去做豆腐。后在他人口中得知一切后，彻底与自家人分离，一走了之。跟随老曾杀猪，被老詹介绍去劈竹子，去染坊挑水，在社火上扮演阎罗被县长看中去县衙里种菜。后又经人介绍，”嫁“给原姜家儿媳妇，寡妇吴香香，自此改名为吴摩西。本想自此好好生活下去，后来发现吴香香和隔壁银铺的掌柜私奔。介于外界压力，和吴香香的女儿巧玲一起假装找吴香香，结果路上巧玲被人拐跑，卖给了无儿无女的老曹。吴摩西在找巧玲的路上遇到了吴香香，自此发现自己的人生一场空。在经过不知道多久的寻找后，吴摩西离开了延津，改名为罗长礼。 下部回延津记远不及上部精彩，主要讲曹青娥也就是巧玲嫁人，生儿育女。曹青娥的二儿子牛爱国的老婆跟别人跑了，历史总是惊人的相似，而后牛爱国又再假借寻找老婆的时候，想了解吴摩西后面的故事，更多是找到自己内心该追求的东西。 上部 出延津记 上部更多的是介绍吴摩西的，杨百顺从小时候崇拜罗长礼，到最后去了改名罗长礼的一系列故事，内心都是希望成为罗长礼这样的喊丧人，喊丧喊得不是声，更是一种呐喊中夹杂着宣泄。上部中出现了几个人物都令人印象深刻。 教书先生老汪：老汪之所以成为教书先生也算是机缘巧合，父亲是个不允许他人获得利益的人，将老汪送入私塾，也是希望老汪可以学有所成，帮助自己与隔壁铺子打官司。但老汪学的并不尽人意，后面被地主聘为先生来教书。老汪的女儿灯盏天生顽皮，不小心坠入水缸中溺亡，老汪本表现的并不异样，只是怪罪其女生性顽皮，直到看见女儿留着月饼上的牙印，抱头痛哭，无法遗忘与女儿的相处。向东家辞去工作，一路向西，从延津到新乡到焦作到洛阳到三门峡，直到出了河南，到了陕西宝鸡才不伤心。便在此落脚，以吹糖人为生。 ","date":"2023-06-21","objectID":"/yi-wang-ju-ding-yi-ju/:4:0","tags":["刘震云"],"title":"《一句话顶一万句话》全书评","uri":"/yi-wang-ju-ding-yi-ju/"},{"categories":null,"content":"关于","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于","uri":"/about/"}]